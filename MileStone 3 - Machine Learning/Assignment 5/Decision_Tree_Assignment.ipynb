{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree | Assignment**"
      ],
      "metadata": {
        "id": "whcZvki2qjdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?**\n",
        "\n",
        "Definition\n",
        "\n",
        "- A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks.\n",
        "\n",
        "- It is a tree-like structure where each internal node represents a feature (attribute), each branch represents a decision (rule) based on that feature, and each leaf node represents an outcome (class label in classification).\n",
        "\n",
        "How it Works in Classification\n",
        "\n",
        "1.Root Node (Start):\n",
        "\n",
        "- The tree starts with the root node, which contains the entire dataset.\n",
        "\n",
        "- The algorithm chooses the best feature to split the data.\n",
        "\n",
        "- Best feature = one that maximizes class separation (using measures like Gini Index, Entropy/Information Gain, or Chi-Square).\n",
        "\n",
        "2.Splitting (Recursive Partitioning):\n",
        "\n",
        "- Data is divided into subsets based on the chosen feature‚Äôs possible values.\n",
        "\n",
        "- This process is repeated recursively, forming branches and child nodes.\n",
        "\n",
        "3.Stopping Criteria:\n",
        "\n",
        "- Splitting continues until one of the following occurs:\n",
        "\n",
        "- All records in a node belong to the same class.\n",
        "\n",
        "- No further improvement can be made by splitting.\n",
        "\n",
        "Maximum depth or minimum samples per leaf is reached (to avoid overfitting).\n",
        "\n",
        "4.Leaf Nodes (Prediction):\n",
        "\n",
        "- Each leaf node represents a final classification.\n",
        "\n",
        "- For a new input, the model traverses the tree according to the feature values until it reaches a leaf node, and assigns the corresponding class label.\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose we want to classify whether a person will buy a laptop (Yes/No) based on:\n",
        "\n",
        "- Age (Young, Middle, Old)\n",
        "\n",
        "- Income (High, Medium, Low)\n",
        "\n",
        "- The tree might first split on Income (best predictor).\n",
        "\n",
        "- If Income = High ‚Üí predict ‚ÄúYes‚Äù.\n",
        "\n",
        "- If Income = Low ‚Üí split further on Age.\n",
        "\n",
        "- Eventually, leaf nodes give the final Yes/No classification.\n",
        "\n",
        "Advantages\n",
        "\n",
        "- Easy to understand and interpret (like human decision-making).\n",
        "\n",
        "- Works well with both categorical and numerical data.\n",
        "\n",
        "- Requires little preprocessing.\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "- Prone to overfitting if not pruned.\n",
        "\n",
        "- Sensitive to small changes in data."
      ],
      "metadata": {
        "id": "2IqWKLi5qcIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "1.Impurity in Decision Trees\n",
        "\n",
        "- In Decision Trees, impurity measures determine how well a feature separates the classes.\n",
        "\n",
        "- A pure node contains only one class ‚Üí impurity = 0.\n",
        "\n",
        "- An impure node contains mixed classes ‚Üí impurity > 0.\n",
        "\n",
        "- The algorithm chooses splits that reduce impurity the most.\n",
        "\n",
        "2.Gini Impurity\n",
        "\n",
        "- Definition: Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if we label it according to the class distribution in the node.\n",
        "\n",
        "- Formula:\n",
        "\n",
        "- $$Gini=1‚àíi=1‚àëk‚Äãpi2‚Äã$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- ùëò = number of classes\n",
        "\n",
        "- ùëùùëñ = probability of class\n",
        "\n",
        "- ùëñ in the node\n",
        "\n",
        "- Range: 0 ‚Üí perfectly pure (only one class)\n",
        "\n",
        "- Maximum = 0.5 (for binary classes with equal probability 0.5, 0.5)\n",
        "\n",
        "Example:\n",
        "\n",
        "- Suppose a node has 70% Class A, 30% Class B.\n",
        "\n",
        "- $Gini=1‚àí(0.72+0.32)=1‚àí(0.49+0.09)=0.42$\n",
        "\n",
        "- ‚Üí Some impurity exists.\n",
        "\n",
        "3.Entropy\n",
        "\n",
        "- Definition: Entropy (from Information Theory) measures the uncertainty or disorder in a node.\n",
        "\n",
        "- Formula:\n",
        "\n",
        "- $Entropy=‚àíi=1‚àëk‚Äãpi‚Äãlog2‚Äã(pi‚Äã)$\n",
        "\n",
        "- Range:\n",
        "\n",
        "- 0 ‚Üí perfectly pure (all samples in one class)\n",
        "\n",
        "- 1 ‚Üí maximum disorder (binary classes equally split, 0.5 & 0.5)\n",
        "\n",
        "- Example:\n",
        "\n",
        "- For the same node (70% A, 30% B):\n",
        "\n",
        "- $Entropy=‚àí(0.7log2‚Äã(0.7)+0.3log2‚Äã(0.3))‚âà0.881$\n",
        "\n",
        "- ‚Üí Shows more disorder than Gini.\n",
        "\n",
        "4.Impact on Splits\n",
        "\n",
        "- Decision Trees use Gini or Entropy to evaluate splits.\n",
        "\n",
        "- For each possible feature split:\n",
        "\n",
        "- Compute the weighted average impurity of child nodes.\n",
        "\n",
        "- Choose the split that minimizes impurity the most (highest Information Gain).\n",
        "\n",
        "Difference:\n",
        "\n",
        "- Gini is faster to compute (no logarithms).\n",
        "\n",
        "- Entropy gives more weight to rare classes.\n",
        "\n",
        "- In practice, both often give similar trees.\n",
        "\n",
        "| Feature                  | **Gini Impurity**                              | **Entropy**                                                 |\n",
        "| ------------------------ | ---------------------------------------------- | ----------------------------------------------------------- |\n",
        "| **Definition**           | Measures the probability of misclassification  | Measures the disorder/uncertainty in the data               |\n",
        "| **Formula**              | $Gini = 1 - \\sum p_i^2$                        | $Entropy = -\\sum p_i \\log_2(p_i)$                           |\n",
        "| **Range**                | 0 (pure) ‚Üí 0.5 (binary max)                    | 0 (pure) ‚Üí 1 (binary max)                                   |\n",
        "| **Computation**          | Faster (no logarithm)                          | Slower (uses logarithm)                                     |\n",
        "| **Interpretation**       | Focuses on correct classification probability  | Focuses on information content and uncertainty              |\n",
        "| **Bias**                 | Slightly favors larger classes                 | Treats rare classes more carefully                          |\n",
        "| **Practical Difference** | Often similar results                          | Often similar results                                       |\n",
        "| **Preferred Use**        | Default in scikit-learn DecisionTreeClassifier | Used when information-theory based interpretation is needed |\n",
        "\n"
      ],
      "metadata": {
        "id": "-2R3Wps6ron4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "\n",
        "1.Introduction\n",
        "\n",
        "- Decision Trees tend to overfit if they grow too deep, capturing noise instead of patterns.\n",
        "\n",
        "- Pruning is a technique used to reduce the size of the tree and improve generalization.\n",
        "\n",
        "- Two main types: Pre-Pruning and Post-Pruning.\n",
        "\n",
        "2.Pre-Pruning (Early Stopping)\n",
        "\n",
        "- Definition: Stop growing the tree before it becomes too complex.\n",
        "\n",
        "- How: By setting constraints like:\n",
        "\n",
        "- Maximum depth of tree (max_depth)\n",
        "\n",
        "- Minimum samples required to split a node (min_samples_split)\n",
        "\n",
        "- Minimum samples per leaf (min_samples_leaf)\n",
        "\n",
        "- Maximum number of leaf nodes (max_leaf_nodes)\n",
        "\n",
        "- Practical Advantage: Saves time and memory during training because the tree does not grow unnecessarily large.\n",
        "\n",
        "3.Post-Pruning (Reduced Error Pruning)\n",
        "\n",
        "- Definition: First grow the tree fully, then remove branches that do not improve performance.\n",
        "\n",
        "How:\n",
        "\n",
        "- Train a full tree.\n",
        "\n",
        "- Use a validation set or cross-validation to evaluate performance.\n",
        "\n",
        "- Iteratively prune subtrees that do not improve accuracy.\n",
        "\n",
        "- Practical Advantage: Produces a more accurate and generalized tree, as pruning is based on actual performance rather than fixed rules.\n",
        "\n",
        "4. Key Difference\n",
        "\n",
        "| Aspect                | **Pre-Pruning**                                | **Post-Pruning**                      |\n",
        "| --------------------- | ---------------------------------------------- | ------------------------------------- |\n",
        "| **When applied**      | During tree construction                       | After full tree is built              |\n",
        "| **Decision criteria** | Based on stopping rules (depth, samples, etc.) | Based on validation performance       |\n",
        "| **Complexity**        | Lower training cost                            | Higher training cost                  |\n",
        "| **Risk**              | May underfit if stopped too early              | Less underfitting, but more expensive |\n",
        "\n",
        "5.Summary\n",
        "\n",
        "- Pre-Pruning ‚Üí Faster, prevents overly deep trees early.\n",
        "\n",
        "- Post-Pruning ‚Üí More accurate, improves generalization by removing weak branches.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "s5rPimjOunVi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs4pdRBPpoXu",
        "outputId": "41aca90d-1ab6-4e13-ea68-e5079f8bcd46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-Pruning Accuracy: 1.0\n",
            "Full Tree Accuracy (before pruning): 1.0\n",
            "Best alpha for post-pruning: 0.0\n",
            "Post-Pruning Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Pre-Pruning Example\n",
        "# -----------------------------\n",
        "# Limit the depth of the tree (pre-pruning)\n",
        "pre_pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pre_pruned_tree.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_pre = pre_pruned_tree.predict(X_test)\n",
        "print(\"Pre-Pruning Accuracy:\", accuracy_score(y_test, y_pred_pre))\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Post-Pruning Example\n",
        "# -----------------------------\n",
        "# First, train a full tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "\n",
        "print(\"Full Tree Accuracy (before pruning):\", accuracy_score(y_test, full_tree.predict(X_test)))\n",
        "\n",
        "# Use Cost-Complexity Pruning (Post-Pruning)\n",
        "# Get effective alphas and impurities\n",
        "path = full_tree.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "# Train different pruned trees for each alpha\n",
        "best_accuracy = 0\n",
        "best_alpha = 0\n",
        "for alpha in ccp_alphas:\n",
        "    pruned_tree = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    pruned_tree.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, pruned_tree.predict(X_test))\n",
        "    if acc > best_accuracy:\n",
        "        best_accuracy = acc\n",
        "        best_alpha = alpha\n",
        "\n",
        "print(\"Best alpha for post-pruning:\", best_alpha)\n",
        "print(\"Post-Pruning Accuracy:\", best_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "1.Introduction\n",
        "\n",
        "- Decision Trees split data at each node to create pure subsets.\n",
        "\n",
        "- Information Gain (IG) is a metric that measures how much ‚Äúinformation‚Äù a feature provides about the class labels after a split.\n",
        "\n",
        "- It is based on the reduction in Entropy (or impurity).\n",
        "\n",
        "2.Definition of Information Gain\n",
        "\n",
        "- Entropy before split: Measures the impurity of the parent node.\n",
        "\n",
        "- Entropy after split: Weighted average of entropy of child nodes.\n",
        "\n",
        "- $$IG(S, A) = Entropy(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\cdot Entropy(S_v)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- ùëÜ = dataset at the node\n",
        "\n",
        "- ùê¥ = feature used for split\n",
        "\n",
        "- ùëÜùë£ = subset of\n",
        "\n",
        "- ùëÜ where feature\n",
        "\n",
        "- ùê¥ = ùë£\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- Higher IG ‚Üí feature provides more useful information ‚Üí better split.\n",
        "\n",
        "- If IG = 0 ‚Üí feature doesn‚Äôt help in classification.\n",
        "\n",
        "3.Example\n",
        "\n",
        "- Suppose we want to classify whether a student will Pass or Fail based on Study Hours.\n",
        "\n",
        "- Parent Node Entropy (before split):\n",
        "6 Pass, 6 Fail ‚Üí\n",
        "\n",
        "- $Entropy=‚àí(0.5log2‚Äã0.5+0.5log2‚Äã0.5)=1.0$\n",
        "\n",
        "- After Split (Study Hours < 5 and ‚â• 5):\n",
        "\n",
        "- Left Node: 5 Fail, 1 Pass ‚Üí Entropy ‚âà 0.65\n",
        "\n",
        "- Right Node: 5 Pass, 1 Fail ‚Üí Entropy ‚âà 0.65\n",
        "\n",
        "Weighted Entropy:\n",
        "\n",
        "- $(6/12√ó0.65)+(6/12√ó0.65)=0.65$\n",
        "\n",
        "Information Gain:\n",
        "\n",
        "- $IG=1.0‚àí0.65=0.35$\n",
        "\n",
        "- This means splitting on Study Hours improves classification.\n",
        "\n",
        "4.Importance of Information Gain\n",
        "\n",
        "- Feature Selection: Helps select the feature that best separates classes.\n",
        "\n",
        "- Tree Growth: Ensures that each split increases ‚Äúpurity.‚Äù\n",
        "\n",
        "- Prevents Random Splits: Without IG, splits may be meaningless.\n",
        "\n",
        "- Better Generalization: IG ensures the tree learns patterns, not noise.\n",
        "\n",
        "5.Summary\n",
        "\n",
        "- Information Gain = Reduction in Entropy after a split.\n",
        "\n",
        "- Higher IG ‚Üí better feature choice.\n",
        "\n",
        "- It ensures that Decision Trees grow accurate and meaningful splits.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "hQwhVGhIvw3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset (Iris flower dataset)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree with Entropy (Information Gain)\n",
        "tree_entropy = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=42)\n",
        "tree_entropy.fit(X_train, y_train)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Decision Tree Accuracy:\", tree_entropy.score(X_test, y_test))\n",
        "\n",
        "# Show tree rules\n",
        "tree_rules = export_text(tree_entropy, feature_names=iris.feature_names)\n",
        "print(\"\\nDecision Tree Rules (using Information Gain):\\n\")\n",
        "print(tree_rules)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjNzisN0yEBl",
        "outputId": "c699ee53-dcb2-43d8-a10a-2df56974af5d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9777777777777777\n",
            "\n",
            "Decision Tree Rules (using Information Gain):\n",
            "\n",
            "|--- petal length (cm) <= 2.45\n",
            "|   |--- class: 0\n",
            "|--- petal length (cm) >  2.45\n",
            "|   |--- petal length (cm) <= 4.75\n",
            "|   |   |--- petal width (cm) <= 1.60\n",
            "|   |   |   |--- class: 1\n",
            "|   |   |--- petal width (cm) >  1.60\n",
            "|   |   |   |--- class: 2\n",
            "|   |--- petal length (cm) >  4.75\n",
            "|   |   |--- petal length (cm) <= 5.15\n",
            "|   |   |   |--- class: 2\n",
            "|   |   |--- petal length (cm) >  5.15\n",
            "|   |   |   |--- class: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?**\n",
        "\n",
        "1.Introduction\n",
        "\n",
        "- Decision Trees are widely used in classification and regression problems because they are simple, interpretable, and mimic human decision-making.\n",
        "\n",
        "2.Real-World Applications (8 marks)\n",
        "\n",
        "A. Healthcare Diagnosis\n",
        "\n",
        "- Predict diseases (e.g., diabetes, cancer) based on patient symptoms, test results, and lifestyle factors.\n",
        "\n",
        "B. Finance & Banking\n",
        "\n",
        "- Credit scoring and loan approval decisions.\n",
        "\n",
        "- Fraud detection in transactions.\n",
        "\n",
        "C. Marketing & Customer Analytics\n",
        "\n",
        "- Predict customer churn (likelihood of leaving a service).\n",
        "\n",
        "- Targeted advertising and product recommendations.\n",
        "\n",
        "D. Manufacturing & Quality Control\n",
        "\n",
        "- Predict product defects based on production conditions.\n",
        "\n",
        "- Optimize supply chain decisions.\n",
        "\n",
        "E. Agriculture\n",
        "\n",
        "- Crop disease detection.\n",
        "\n",
        "- Yield prediction based on soil quality, weather, and irrigation.\n",
        "\n",
        "F. Education\n",
        "\n",
        "- Predict student performance based on study habits and attendance.\n",
        "\n",
        "3.Advantages of Decision Trees\n",
        "\n",
        "- Easy to Understand & Interpret ‚Äì Mimics human reasoning with ‚Äúif-then‚Äù rules.\n",
        "\n",
        "- No Need for Feature Scaling ‚Äì Works well with raw data (no normalization/standardization required).\n",
        "\n",
        "- Handles Both Numerical & Categorical Data.\n",
        "\n",
        "- Fast Training & Prediction ‚Äì Efficient on small-to-medium datasets.\n",
        "\n",
        "- Feature Selection Built-In ‚Äì Automatically picks the most important features.\n",
        "\n",
        "4.Limitations of Decision Trees\n",
        "\n",
        "- Overfitting ‚Äì Tends to create very deep trees that don‚Äôt generalize well.\n",
        "\n",
        "- Unstable ‚Äì Small changes in data can produce a very different tree.\n",
        "\n",
        "- Biased Splits ‚Äì Features with many categories may dominate splits.\n",
        "\n",
        "- Not Great for Continuous Predictions ‚Äì Regression trees are less accurate than advanced models.\n",
        "\n",
        "- Lower Accuracy than Ensemble Models ‚Äì Alone, Decision Trees are weaker than Random Forests or Gradient Boosted Trees.\n",
        "\n",
        "Applications of Decision Trees\n",
        "\n",
        "| **Domain**                | **Example Use Case**                                                  |\n",
        "| ------------------------- | --------------------------------------------------------------------- |\n",
        "| **Healthcare**          | Predict diseases (e.g., diabetes, cancer) based on symptoms and tests |\n",
        "| **Finance & Banking**   | Loan approval, credit scoring, fraud detection                        |\n",
        "| **Marketing & Retail**  | Predict customer churn, product recommendation, targeted ads          |\n",
        "| **Manufacturing**       | Detect defective products, optimize supply chain                      |\n",
        "| **Agriculture**        | Crop disease detection, yield prediction                              |\n",
        "| **Education**           | Predict student performance, dropout risk analysis                    |\n",
        "\n",
        "\n",
        "5.Summary (Importance)\n",
        "\n",
        "- Applications: Healthcare, finance, marketing, manufacturing, etc.\n",
        "\n",
        "- Advantages: Simple, interpretable, no preprocessing needed.\n",
        "\n",
        "- Limitations: Overfitting, instability, less accurate compared to ensembles."
      ],
      "metadata": {
        "id": "St1R6L16yKWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Write a Python program to:**\n",
        "\n",
        "**‚óè Load the Iris Dataset**\n",
        "\n",
        "**‚óè Train a Decision Tree Classifier using the Gini criterion**\n",
        "\n",
        "**‚óè Print the model‚Äôs accuracy and feature importances**\n",
        "\n",
        "Answer :"
      ],
      "metadata": {
        "id": "yeM8r_xEzTdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy (Gini):\", accuracy)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmBK8djR0Loc",
        "outputId": "fe31938f-d7fb-4537-ed3a-ffa7294ffe4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy (Gini): 1.0\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Write a Python program to:**\n",
        "\n",
        "**‚óè Load the Iris Dataset**\n",
        "\n",
        "**‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.**\n",
        "\n",
        "Answer :"
      ],
      "metadata": {
        "id": "yq6P5Udp0RGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Fully-grown Decision Tree (no depth limit)\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# 2. Decision Tree with max_depth=3\n",
        "shallow_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "shallow_tree.fit(X_train, y_train)\n",
        "y_pred_shallow = shallow_tree.predict(X_test)\n",
        "accuracy_shallow = accuracy_score(y_test, y_pred_shallow)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of Fully-grown Tree:\", accuracy_full)\n",
        "print(\"Accuracy of Tree with max_depth=3:\", accuracy_shallow)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gPF1nvT0ijg",
        "outputId": "56b761bd-bb24-4aea-a662-f1972614d5a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Fully-grown Tree: 1.0\n",
            "Accuracy of Tree with max_depth=3: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Write a Python program to:**\n",
        "\n",
        "**‚óè Load the California Housing dataset from sklearn**\n",
        "\n",
        "**‚óè Train a Decision Tree Regressor**\n",
        "\n",
        "**‚óè Print the Mean Squared Error (MSE) and feature importances**\n",
        "\n",
        "Answer :"
      ],
      "metadata": {
        "id": "aca5OmHv0oDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2ThsBwv04sK",
        "outputId": "28b353c2-dfeb-4d36-aca2-7aa24adacdc2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280096503174904\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5235\n",
            "HouseAge: 0.0521\n",
            "AveRooms: 0.0494\n",
            "AveBedrms: 0.0250\n",
            "Population: 0.0322\n",
            "AveOccup: 0.1390\n",
            "Latitude: 0.0900\n",
            "Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Write a Python program to:**\n",
        "\n",
        "**‚óè Load the Iris Dataset**\n",
        "\n",
        "**‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "GridSearchCV**\n",
        "\n",
        "**‚óè Print the best parameters and the resulting model accuracy**\n",
        "\n",
        "Answer :"
      ],
      "metadata": {
        "id": "XpHLiGJ509km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Best estimator accuracy on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy on Test Set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjqTc-Vj1PIp",
        "outputId": "9a5872e7-4ff8-486f-e515-0a5e786182a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy on Test Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Imagine you‚Äôre working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:**\n",
        "\n",
        "**‚óè Handle the missing values**\n",
        "\n",
        "**‚óè Encode the categorical features**\n",
        "\n",
        "**‚óè Train a Decision Tree model**\n",
        "\n",
        "**‚óè Tune its hyperparameters**\n",
        "\n",
        "**‚óè Evaluate its performance**\n",
        "\n",
        "**And describe what business value this model could provide in the real-world setting.**\n",
        "\n",
        "High-level summary (one line)\n",
        "\n",
        "- Build a robust pipeline: clean & impute ‚Üí encode ‚Üí (optionally) balance classes ‚Üí train with cross-validation ‚Üí tune ‚Üí calibrate ‚Üí evaluate on hold-out & external data ‚Üí explain & deploy with monitoring.\n",
        "\n",
        "1.Handle missing values (why & how)\n",
        "\n",
        "- Understand the missingness: check whether values are MCAR / MAR / MNAR ‚Äî that affects choice of imputation (e.g., MNAR needs domain input).\n",
        "\n",
        "- Simple, safe imputation first:\n",
        "\n",
        "- For numeric features: median (robust to outliers) or model-based / iterative imputation if patterns are complex.\n",
        "\n",
        "- For categorical: most_frequent or a special \"MISSING\" category.\n",
        "\n",
        "- Always add missingness indicator (binary flag per feature) to preserve signal that a value was missing ‚Äî this often helps predictive models. (This approach is used and demonstrated in sklearn‚Äôs imputation examples).\n",
        "Scikit-learn\n",
        "+1\n",
        "\n",
        "- Advanced imputation when needed:\n",
        "\n",
        "- Use IterativeImputer / KNN imputation if missingness relates to other features and dataset size supports it.\n",
        "\n",
        "- Do imputation inside a pipeline (never impute on full data before cross-validation) ‚Äî this avoids data leakage.\n",
        "\n",
        "2.Encode categorical features (practical choices)\n",
        "\n",
        "- Low-cardinality categories: use OneHotEncode (handle_unknown='ignore').\n",
        "\n",
        "- High-cardinality categorical variables (e.g., thousands of codes): prefer target encoding / frequency encoding or categorical embeddings (if using tree-based libraries that accept categories) ‚Äî but target encoding must be done inside CV to avoid leakage.\n",
        "\n",
        "- Ordinal categorical variables: if an intrinsic order exists, use OrdinalEncoder (with care).\n",
        "\n",
        "- Pipeline approach: use ColumnTransformer to apply numeric and categorical pipelines separately.\n",
        "\n",
        "3.Balance classes (important for disease prediction)\n",
        "\n",
        "- If disease prevalence is low, the model can be biased toward the majority (healthy). Two common options:\n",
        "\n",
        "- Resampling (applied only on training folds): SMOTE (synthetic minority oversampling) and its variants (SMOTE + Tomek, SMOTEENN) ‚Äî available in imblearn. Use inside the training pipeline so resampling happens per CV fold.\n",
        "imbalanced-learn.org\n",
        "\n",
        "- Class weights: set class_weight='balanced' in DecisionTreeClassifier to upweight minority class without changing sample counts. Good when synthetic samples might be risky.\n",
        "\n",
        "- Which to choose? If minority patterns are under-represented in feature space, SMOTE often helps. If you want simplicity and avoid synthetic patients, use class_weight. (Both are valid; try both in tuning.)\n",
        "\n",
        "4.Train a Decision Tree model (practical setup)\n",
        "\n",
        "- Prefer building a scikit-learn pipeline that encapsulates preprocessing ‚Üí (resampler) ‚Üí classifier so all steps are applied correctly inside cross-validation.\n",
        "\n",
        "- Important tree hyperparameters to consider: max_depth, min_samples_split, min_samples_leaf, max_features, and pruning parameter ccp_alpha (cost-complexity pruning). These control complexity and overfitting.\n",
        "\n",
        "5.Hyperparameter tuning\n",
        "\n",
        "- Use StratifiedKFold (preserve class ratios in each fold).\n",
        "\n",
        "- Use GridSearchCV for small grids, or RandomizedSearchCV for larger search spaces. Tune with scoring that matches business needs (see next section).\n",
        "\n",
        "- If dataset is large or hyperparameter space is big, consider Bayesian optimization (Optuna, scikit-optimize).\n",
        "\n",
        "- When resampling (SMOTE) is used, put it inside the pipeline so sampling is done only on training folds.\n",
        "\n",
        "6.Evaluation (what to measure ‚Äî this is critical in healthcare)\n",
        "\n",
        "- Do not rely on accuracy for rare disease prediction. Use:\n",
        "\n",
        "- Recall (sensitivity) ‚Äî how many actual diseased patients we detect (critical when missing a disease is costly).\n",
        "\n",
        "- Precision ‚Äî among predicted positives, how many truly have disease (important to avoid unnecessary followups).\n",
        "\n",
        "- F1-score ‚Äî harmonic mean (useful when balancing precision & recall).\n",
        "\n",
        "- ROC-AUC and PR-AUC (average precision) ‚Äî for imbalanced data PR-AUC / precision-recall curves can be more informative than ROC.\n",
        "PLOS\n",
        "\n",
        "- Confusion matrix for concrete counts.\n",
        "\n",
        "- Calibration (do predicted probabilities reflect true risk?) ‚Äî produce calibration curve / Brier score and consider CalibratedClassifierCV (isotonic or sigmoid) if probabilities are poorly calibrated. scikit-learn provides calibration tools.\n",
        "Scikit-learn\n",
        "\n",
        "- Validation strategy:\n",
        "\n",
        "- Hold out a final test set (never used in CV).\n",
        "\n",
        "- If possible, validate on an external / temporal / prospective dataset to estimate real-world performance.\n",
        "\n",
        "- Decision threshold tuning: choose probability threshold based on business cost matrix (false negatives vs false positives). Use decision/utility analysis (net benefit) to pick thresholds.\n",
        "\n",
        "7.Explainability & model checks\n",
        "\n",
        "- Trees are interpretable ‚Äî show feature_importances_. But for deeper insight, use SHAP or partial dependence plots so clinicians can understand per-patient contributions. (SHAP is commonly used in healthcare explainability literature.)\n",
        "\n",
        "- Check for spurious features (data leakage), fairness (does performance vary by subgroups?), and robustness (sensitivity to small perturbations).\n",
        "\n",
        "8.Calibration & post-processing\n",
        "\n",
        "- Calibrate predicted probabilities if you will use them for risk thresholds / triage (see scikit-learn calibration docs). Calibrated probabilities are essential when the output is used to trigger actions (e.g., immediate referral).\n",
        "Scikit-learn\n",
        "\n",
        "9.Deployment, monitoring & regulatory/ethical considerations\n",
        "\n",
        "- Monitoring: track data drift, performance decay, and input distributions; periodically retrain with recent data.\n",
        "\n",
        "- Logging and auditability: log predictions, outcomes (when available), and model inputs for audits.\n",
        "\n",
        "- Clinical review & governance: model should be reviewed by clinicians and integrated into workflows with human oversight.\n",
        "\n",
        "- Regulatory: medical AI may be regulated (FDA has published an AI/ML action plan and guidance for software as a medical device) ‚Äî follow regulatory best practices (traceability, documentation, risk assessment).\n",
        "U.S. Food and Drug Administration\n",
        "\n",
        "10.Business value (what this model provides)\n",
        "\n",
        "- Early detection / triage: identify high-risk patients for earlier testing or treatment ‚Üí improved outcomes.\n",
        "\n",
        "- Resource prioritization: focus limited diagnostics/treatment resources on the most likely patients.\n",
        "\n",
        "- Cost reduction: reduce unnecessary tests for low-risk patients and avoid late-stage expensive treatments.\n",
        "\n",
        "- Clinical decision support: augment clinicians with an explainable risk score, improving decision consistency.\n",
        "\n",
        "- Population health insights: discover risk factors and actionable interventions across cohorts.\n",
        "\n",
        "- Important caveat: clinical deployment must weigh harms: false negatives (missed disease) can be fatal; false positives can cause anxiety/over-treatment and cost. Choose thresholds with clinicians and quantify net benefit.\n",
        "\n",
        "11.Concise practical checklist (for an assignment / exam ‚Äî 20-mark style)\n",
        "\n",
        "- Inspect missingness ‚Üí decide MCAR/MAR/MNAR ‚Üí impute inside pipeline + add missing flags.\n",
        "Scikit-learn\n",
        "\n",
        "- Encode categorical features via ColumnTransformer (OneHot / target encoding if high cardinality).\n",
        "\n",
        "- Split: train / validation (CV) / held-out test (and external if possible).\n",
        "\n",
        "- Use an imbalance strategy: SMOTE inside pipeline or class_weight='balanced'.\n",
        "imbalanced-learn.org\n",
        "\n",
        "- Build pipeline: preprocess ‚Üí (SMOTE) ‚Üí DecisionTreeClassifier (or ensemble).\n",
        "\n",
        "- Tune max_depth, min_samples_split, min_samples_leaf, max_features, ccp_alpha by GridSearchCV/RandomizedSearchCV with StratifiedKFold.\n",
        "\n",
        "- Evaluate with recall, precision, F1, PR-AUC, ROC-AUC and calibration curves; choose threshold based on cost/benefit.\n",
        "PLOS\n",
        "+1\n",
        "\n",
        "- Explainability: feature importances + SHAP; clinical validation & prospective testing.\n",
        "\n",
        "- Document, monitor, and follow regulatory guidance.\n",
        "U.S. Food and Drug Administration\n",
        "\n",
        "12.Final practical tips\n",
        "\n",
        "- Prefer recall for dangerous diseases (but quantify false positive cost).\n",
        "\n",
        "- Calibrate probabilities before deploying (use CalibratedClassifierCV from sklearn).\n",
        "Scikit-learn\n",
        "\n",
        "- Document everything (data lineage, feature definitions, versions) for audits and regulatory review. FDA has specific guidance and plans for AI/ML SaMD ‚Äî read and follow applicable regulations.  U.S. Food and Drug Administration\n",
        "\n",
        "Key authoritative references (most load-bearing)\n",
        "\n",
        "- scikit-learn imputation & examples (SimpleImputer; missingness indicator example).\n",
        "Scikit-learn\n",
        "+1\n",
        "\n",
        "- SMOTE & imbalanced-learn (resampling techniques for class imbalance).\n",
        "imbalanced-learn.org\n",
        "\n",
        "- Precision-Recall is more informative than ROC for imbalanced datasets (Saito & Rehmsmeier, PLoS One 2015).\n",
        "PLOS\n",
        "\n",
        "- scikit-learn probability calibration (CalibratedClassifierCV, calibration curves).\n",
        "Scikit-learn\n",
        "\n",
        "- FDA AI/ML action plan & regulation considerations for medical software.\n",
        "U.S. Food and Drug Administration\n",
        "\n",
        "13.Colab / sklearn pipeline (runnable skeleton)"
      ],
      "metadata": {
        "id": "ZVFIzO581UOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
        "\n",
        "# ---- Load dataset ----\n",
        "iris = load_iris(as_frame=True)\n",
        "df = iris.frame\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Example: here all features are numeric\n",
        "numeric_cols = X.columns.tolist()\n",
        "categorical_cols = []   # no categorical columns in Iris\n",
        "\n",
        "# ---- Define transformers ----\n",
        "numeric_transformer = ImbPipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = ImbPipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# ---- ColumnTransformer ----\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# ---- Pipeline ----\n",
        "pipeline = ImbPipeline([\n",
        "    ('preproc', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', DecisionTreeClassifier(class_weight='balanced', random_state=42))\n",
        "])\n",
        "\n",
        "# ---- Hyperparameter grid ----\n",
        "param_grid = {\n",
        "    'clf__max_depth': [3, 5, None],\n",
        "    'clf__min_samples_split': [2, 5, 10],\n",
        "    'clf__min_samples_leaf': [1, 2, 5]\n",
        "}\n",
        "\n",
        "# ---- Train/test split ----\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# ---- GridSearchCV ----\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid = GridSearchCV(pipeline, param_grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# ---- Evaluate ----\n",
        "best = grid.best_estimator_\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "y_pred = best.predict(X_test)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wumkzj91wEu",
        "outputId": "dff459b1-3118-41a0-9958-f59ec781ef61"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'clf__max_depth': 3, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2}\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        15\n",
            "           1       0.94      1.00      0.97        15\n",
            "           2       1.00      0.93      0.97        15\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.98      0.98        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n"
          ]
        }
      ]
    }
  ]
}