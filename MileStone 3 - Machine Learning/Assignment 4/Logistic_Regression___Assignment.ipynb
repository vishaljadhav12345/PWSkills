{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression | Assignment**"
      ],
      "metadata": {
        "id": "lC6AfWzcj8sZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Logistic Regression, and how does it differ from Linear Regression?**\n",
        "\n",
        "1.Logistic Regression:\n",
        "\n",
        "- Logistic Regression is a statistical machine learning algorithm used for classification problems.\n",
        "\n",
        "- It predicts the probability that a given input belongs to a certain class (e.g., yes/no, spam/not spam).\n",
        "\n",
        "- Instead of outputting continuous values, it outputs values between 0 and 1, using the sigmoid (logistic) function.\n",
        "\n",
        "- If the probability â‰¥ 0.5 â†’ class = 1, otherwise class = 0.\n",
        "\n",
        "Mathematical Representation:\n",
        "\n",
        "- $$\n",
        "P(Y=1âˆ£X)=1+eâˆ’(Î²0â€‹+Î²1â€‹X1â€‹+...+Î²nâ€‹Xnâ€‹)1\n",
        "â€‹$$\n",
        "\n",
        "2.Linear Regression:\n",
        "\n",
        "- Linear Regression is used for regression problems where the output variable is continuous (e.g., predicting house prices, salary, sales).\n",
        "\n",
        "- It assumes a linear relationship between the dependent variable (Y) and independent variables (X).\n",
        "\n",
        "Equation:\n",
        "\n",
        "- $$\n",
        " Y=Î²0â€‹+Î²1â€‹X1â€‹+Î²2â€‹X2â€‹+...+Î²nâ€‹Xnâ€‹+Ïµ\n",
        " $$\n",
        "\n",
        "3.Key Differences:\n",
        "\n",
        "| Aspect            | Linear Regression                            | Logistic Regression                                 |\n",
        "| ----------------- | -------------------------------------------- | --------------------------------------------------- |\n",
        "| **Purpose**       | Predicts **continuous values**               | Predicts **categorical values** (0/1, Yes/No)       |\n",
        "| **Output**        | Any real number (-âˆ to +âˆ)                   | Probability between **0 and 1**                     |\n",
        "| **Function Used** | Straight line equation                       | Sigmoid (logistic) function                         |\n",
        "| **Error Term**    | Minimizes **Mean Squared Error (MSE)**       | Uses **Maximum Likelihood Estimation (MLE)**        |\n",
        "| **Use Cases**     | Predicting house prices, sales, stock prices | Spam detection, disease diagnosis, churn prediction |\n",
        "\n",
        "4.Example:\n",
        "\n",
        "- Linear Regression Example:\n",
        "Predicting house price based on area.\n",
        "Input â†’ 2000 sq. ft â†’ Output â†’ â‚¹50 lakhs.\n",
        "\n",
        "- Logistic Regression Example:\n",
        "Predicting whether a patient has diabetes (Yes/No).\n",
        "Input â†’ age, BMI, sugar level â†’ Output â†’ Probability = 0.87 â†’ Class = \"Yes\"."
      ],
      "metadata": {
        "id": "ARxGgVGqkAou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Explain the role of the Sigmoid function in Logistic Regression.**\n",
        "\n",
        "1.What is the Sigmoid Function?\n",
        "\n",
        "- The Sigmoid function (also called the logistic function) is a mathematical function that maps any real-valued number into the range (0, 1).\n",
        "\n",
        "- Formula:\n",
        "\n",
        "- $$ Ïƒ(z)=1+eâˆ’z1 â€‹$$\n",
        "\n",
        "- Where\n",
        "\n",
        "- $$ z=Î²0â€‹+Î²1â€‹X1â€‹+Î²2â€‹X2â€‹+...+Î²nâ€‹Xnâ€‹ $$\n",
        "\n",
        "2.Why Sigmoid in Logistic Regression?\n",
        "\n",
        "- Logistic Regression predicts the probability that an observation belongs to a certain class.\n",
        "\n",
        "- Since probabilities must always lie between 0 and 1, we cannot use a simple linear equation (which can give negative or >1 values).\n",
        "\n",
        "- The Sigmoid function compresses any real number into [0,1], making it ideal for probability estimation.\n",
        "\n",
        "3.Role in Classification\n",
        "\n",
        "- Output of Sigmoid = Probability of belonging to class 1.\n",
        "\n",
        "- Decision rule:\n",
        "\n",
        "- If ğœ(ğ‘§)â‰¥0.5 â†’ Predict class = 1\n",
        "\n",
        "- If ğœ(ğ‘§)< 0.5 â†’ Predict class = 0\n",
        "\n",
        "\n",
        "4.Example\n",
        "\n",
        "Suppose a Logistic Regression model gives:\n",
        "\n",
        "- ğ‘§ = 2.3\n",
        "\n",
        "Applying sigmoid:\n",
        "\n",
        "- $$ Ïƒ(2.3)=1+eâˆ’2.31â€‹â‰ˆ0.91 $$\n",
        "\n",
        "- Interpretation â†’ The model predicts a 91% chance of being in Class 1.\n",
        "\n",
        "5.Key Points:\n",
        "\n",
        "- Sigmoid ensures outputs are valid probabilities.\n",
        "\n",
        "- Acts as a link function in Logistic Regression.\n",
        "\n",
        "- Provides a decision boundary for classification.\n",
        "\n",
        "- Smooth and differentiable, which helps in optimization (Maximum Likelihood Estimation).\n"
      ],
      "metadata": {
        "id": "NcmaElBLmHz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What is Regularization in Logistic Regression and why is it needed?**\n",
        "\n",
        "1.Definition of Regularization\n",
        "\n",
        "- Regularization is a technique used in Logistic Regression (and other ML models) to prevent overfitting by adding a penalty term to the cost (loss) function.\n",
        "\n",
        "- It discourages the model from fitting too closely to the training data and keeps the coefficients (Î²) small and stable.\n",
        "\n",
        "2.Why is Regularization Needed?\n",
        "\n",
        "- In Logistic Regression, if we have too many features, the model may assign very large weights to some of them.\n",
        "\n",
        "- This leads to overfitting:\n",
        "\n",
        "- Good performance on training data.\n",
        "\n",
        "- Poor performance on unseen/test data.\n",
        "\n",
        "- Regularization reduces model complexity and improves generalization.\n",
        "\n",
        "3.Types of Regularization in Logistic Regression\n",
        "\n",
        "- L1 Regularization (Lasso):\n",
        "\n",
        "- Adds the absolute values of coefficients as a penalty.\n",
        "\n",
        "- Cost function:\n",
        "\n",
        "- $$J(Î²)=âˆ’m1â€‹i=1âˆ‘mâ€‹[yiâ€‹log(hÎ²â€‹(xiâ€‹))+(1âˆ’yiâ€‹)log(1âˆ’hÎ²â€‹(xiâ€‹))]+Î»j=1âˆ‘nâ€‹âˆ£Î²jâ€‹âˆ£$$\n",
        "\n",
        "- Advantage: Performs feature selection (some coefficients become 0).\n",
        "\n",
        "- L2 Regularization (Ridge):\n",
        "\n",
        "- Adds the squared values of coefficients as a penalty.\n",
        "\n",
        "- Cost function:\n",
        "\n",
        "- $$J(Î²)=âˆ’m1â€‹i=1âˆ‘mâ€‹[yiâ€‹log(hÎ²â€‹(xiâ€‹))+(1âˆ’yiâ€‹)log(1âˆ’hÎ²â€‹(xiâ€‹))]+Î»j=1âˆ‘nâ€‹Î²j2â€‹$$\n",
        "\n",
        "- Advantage: Prevents coefficients from becoming too large.\n",
        "\n",
        "- Elastic Net:\n",
        "\n",
        "- Combination of L1 and L2.\n",
        "\n",
        "4.Key Benefits\n",
        "\n",
        "- Controls model complexity.\n",
        "\n",
        "- Prevents overfitting.\n",
        "\n",
        "- Improves prediction accuracy on unseen data.\n",
        "\n",
        "- Can perform feature selection (L1).\n",
        "\n",
        "5.Example\n",
        "\n",
        "- Without regularization: Model predicts perfectly on training data but fails on test data (overfit).\n",
        "\n",
        "- With L2 regularization: Coefficients shrink â†’ smoother decision boundary â†’ better test accuracy."
      ],
      "metadata": {
        "id": "JPiZmM3MoLME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What are some common evaluation metrics for classification models, and why are they important?**\n",
        "\n",
        "1.Why Evaluation Metrics are Important?\n",
        "\n",
        "- In classification, accuracy alone is not enough (especially when data is imbalanced).\n",
        "\n",
        "- Evaluation metrics help us measure:\n",
        "\n",
        "- How well the model predicts correctly\n",
        "\n",
        "- Whether it misclassifies certain classes more than others\n",
        "\n",
        "- Overall performance and fairness of the model\n",
        "\n",
        "2.Common Evaluation Metrics\n",
        "\n",
        "- Accuracy\n",
        "\n",
        "- Formula:\n",
        "\n",
        "- Accuracy = $$TP+TN / TP+TN+FP+FN $$\n",
        "\n",
        "- Measures the percentage of correctly classified samples.\n",
        "\n",
        "- Limitation: Misleading when classes are imbalanced.\n",
        "\n",
        "- Precision (Positive Predictive Value)\n",
        "\n",
        "- Formula:\n",
        "\n",
        "- Precision = $$ ğ‘‡ğ‘ƒ / ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ$$\n",
        "\n",
        "- Out of all predicted positives, how many are actually positive.\n",
        "\n",
        "- Useful in applications where false positives are costly (e.g., spam detection).\n",
        "\n",
        "3.Recall (Sensitivity or True Positive Rate)\n",
        "\n",
        "- Formula:\n",
        "\n",
        "- Recall = $$ğ‘‡ğ‘ƒ / ğ‘‡ğ‘ƒ+ğ¹ğ‘$$\n",
        "\tâ€‹\n",
        "- Out of all actual positives, how many are correctly predicted.\n",
        "\n",
        "- Important when missing a positive case is costly (e.g., cancer detection).\n",
        "\n",
        "4.F1 Score\n",
        "\n",
        "- Formula:\n",
        "\n",
        "- $$F1=2Ã—Precision*Recall / Precision+Recallâ€‹$$\n",
        "\tâ€‹\n",
        "- Harmonic mean of Precision and Recall.\n",
        "\n",
        "- Useful when there is class imbalance.\n",
        "\n",
        "5.ROC Curve & AUC (Area Under Curve)\n",
        "\n",
        "- ROC curve plots True Positive Rate vs. False Positive Rate.\n",
        "\n",
        "- AUC measures the overall ability of the model to separate classes.\n",
        "\n",
        "- Higher AUC = better classifier.\n",
        "\n",
        "6.Example (Binary Classification)\n",
        "\n",
        "- Model predicting whether an email is spam or not:\n",
        "\n",
        "- Accuracy: Overall correct predictions.\n",
        "\n",
        "- Precision: Of emails marked as spam, how many are actually spam.\n",
        "\n",
        "- Recall: Of all spam emails, how many were detected.\n",
        "\n",
        "- F1 Score: Balance between precision and recall.\n",
        "\n",
        "- AUC: How well the model distinguishes spam vs. non-spam.  \n",
        "\n",
        "7.Key Points\n",
        "\n",
        "- Accuracy is not always reliable (imbalanced datasets).\n",
        "\n",
        "- Precision & Recall explain false positives vs. false negatives.\n",
        "\n",
        "- F1 balances precision and recall.\n",
        "\n",
        "- AUC measures separability of classes.\n"
      ],
      "metadata": {
        "id": "6jxuzRr4pvjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**\n",
        "\n",
        "Dataset Used\n",
        "\n",
        "- Breast Cancer Wisconsin dataset (load_breast_cancer) from sklearn.datasets.\n",
        "\n",
        "- Features: tumor measurements (mean radius, mean texture, etc.)\n",
        "\n",
        "- Target: 0 = malignant, 1 = benign.\n",
        "\n",
        "Python Program â€“ Logistic Regression with Train/Test Split\n"
      ],
      "metadata": {
        "id": "kOqTTuXJscDV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U2olwDqiegu",
        "outputId": "9ae84d6e-530e-4e95-e855-5927838be22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (569, 31)\n",
            "First 5 rows:\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33           184.60      2019.0   \n",
            "1                 0.05667  ...          23.41           158.80      1956.0   \n",
            "2                 0.05999  ...          25.53           152.50      1709.0   \n",
            "3                 0.09744  ...          26.50            98.87       567.7   \n",
            "4                 0.05883  ...          16.67           152.20      1575.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  target  \n",
            "0          0.4601                  0.11890       0  \n",
            "1          0.2750                  0.08902       0  \n",
            "2          0.3613                  0.08758       0  \n",
            "3          0.6638                  0.17300       0  \n",
            "4          0.2364                  0.07678       0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "Model Accuracy: 0.9561\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target   # Add target column\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"First 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# 2. Define features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=5000)  # Increased iterations for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**\n",
        "\n",
        "Explanation of Key Points\n",
        "\n",
        "- Dataset: Breast Cancer (from sklearn.datasets).\n",
        "\n",
        "- Regularization: penalty='l2' applies Ridge Regularization (default in sklearn).\n",
        "\n",
        "- Coefficients: model.coef_ shows feature weights after shrinkage.\n",
        "\n",
        "- Accuracy: Evaluates performance on test set.\n",
        "\n",
        "This program covers all requirements:\n",
        "\n",
        "- Uses L2 regularization\n",
        "\n",
        "- Prints coefficients & intercept\n",
        "\n",
        "- Prints accuracy\n",
        "\n",
        "Logistic Regression with L2 Regularization (Ridge)\n"
      ],
      "metadata": {
        "id": "_flLc67-tXEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and Target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 2. Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Logistic Regression with L2 Regularization\n",
        "# (penalty='l2' is default, solver='liblinear' or 'lbfgs' works)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Print Coefficients and Accuracy\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)   # array of feature weights\n",
        "print(\"\\nIntercept:\", model.intercept_)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ynkEJPt3Xd",
        "outputId": "4784f81d-cce0-449b-cf5f-a54d6eba3545"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "[[ 2.13248406e+00  1.52771940e-01 -1.45091255e-01 -8.28669349e-04\n",
            "  -1.42636015e-01 -4.15568847e-01 -6.51940282e-01 -3.44456106e-01\n",
            "  -2.07613380e-01 -2.97739324e-02 -5.00338038e-02  1.44298427e+00\n",
            "  -3.03857384e-01 -7.25692126e-02 -1.61591524e-02 -1.90655332e-03\n",
            "  -4.48855442e-02 -3.77188737e-02 -4.17516190e-02  5.61347410e-03\n",
            "   1.23214996e+00 -4.04581097e-01 -3.62091502e-02 -2.70867580e-02\n",
            "  -2.62630530e-01 -1.20898539e+00 -1.61796947e+00 -6.15250835e-01\n",
            "  -7.42763610e-01 -1.16960181e-01]]\n",
            "\n",
            "Intercept: [0.40847797]\n",
            "\n",
            "Model Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**\n",
        "\n",
        "Explanation\n",
        "\n",
        "- Dataset: Iris (3 classes â†’ Setosa, Versicolor, Virginica).\n",
        "\n",
        "- multi_class='ovr': Builds one Logistic Regression classifier per class (One-vs-Rest).\n",
        "\n",
        "- classification_report: Shows precision, recall, f1-score, and support for each class.\n",
        "\n",
        "This program covers all requirements:\n",
        "\n",
        "- Uses Logistic Regression for multiclass\n",
        "\n",
        "- multi_class='ovr' implemented\n",
        "\n",
        "- Prints classification report\n",
        "\n",
        "Logistic Regression for Multiclass Classification (OvR)\n"
      ],
      "metadata": {
        "id": "eG2zUJM-t7Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Target classes:\", data.target_names)\n",
        "\n",
        "# 2. Features (X) and Target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Logistic Regression with OvR (One-vs-Rest)\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Print Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MT_KsFFuXyf",
        "outputId": "90b06ede-0dc3-406e-94fa-5720268e969f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (150, 5)\n",
            "Target classes: ['setosa' 'versicolor' 'virginica']\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation accuracy.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**\n",
        "\n",
        "Explanation\n",
        "\n",
        "- Dataset: Breast Cancer (binary classification).\n",
        "\n",
        "- C: Inverse of regularization strength. Smaller C = stronger regularization.\n",
        "\n",
        "- penalty: 'l1' (Lasso), 'l2' (Ridge).\n",
        "\n",
        "- GridSearchCV: Performs cross-validation to find the best hyperparameters.\n",
        "\n",
        "- Best model: Evaluated on test set.\n",
        "\n",
        "This program covers all requirements:\n",
        "\n",
        "- Applies GridSearchCV\n",
        "\n",
        "- Tunes C and penalty\n",
        "\n",
        "- Prints best parameters & validation accuracy\n",
        "\n",
        "- Evaluates on test data\n",
        "\n",
        "Logistic Regression with GridSearchCV for Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "197UbjEXuapV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and Target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 2. Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=5000, solver='liblinear')\n",
        "\n",
        "# 4. Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],        # Regularization strength\n",
        "    'penalty': ['l1', 'l2']              # L1 (Lasso), L2 (Ridge)\n",
        "}\n",
        "\n",
        "# 5. GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,              # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# 7. Evaluate on test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeX1afLMu9zP",
        "outputId": "d46091ab-4813-4741-eb50-30df70a3c7d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'penalty': 'l1'}\n",
            "Best Cross-Validation Accuracy: 0.9670\n",
            "Test Accuracy: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.**\n",
        "\n",
        "**(Use Dataset from sklearn package)**\n",
        "\n",
        "Explanation\n",
        "\n",
        "- StandardScaler: Transforms features so that they have mean = 0 and standard deviation = 1.\n",
        "\n",
        "- Logistic Regression (especially with regularization) can perform better when features are on the same scale.\n",
        "\n",
        "- We train two models:\n",
        "\n",
        "  - Without scaling\n",
        "\n",
        "  - With scaling\n",
        "\n",
        "- Then compare accuracies.\n",
        "\n",
        "This program covers:\n",
        "\n",
        "- Loads dataset from sklearn\n",
        "\n",
        "- Splits train/test sets\n",
        "\n",
        "- Trains Logistic Regression with & without scaling\n",
        "\n",
        "- Compares accuracies\n",
        "\n",
        "Logistic Regression with and without Standardization"
      ],
      "metadata": {
        "id": "mF3BtQG8vKkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and Target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 2. Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=5000, solver='liblinear')\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# 4. Logistic Regression with scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=5000, solver='liblinear')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# 5. Compare results\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIDIDLUjvpdE",
        "outputId": "96e17fd8-b10b-4687-cbe2-2f3899871dc4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9561\n",
            "Accuracy with scaling:    0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach youâ€™d take to build a Logistic Regression model â€” including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business use case.**\n",
        "\n",
        "1.Understanding the Problem\n",
        "\n",
        "- Business goal: Predict which customers will respond to a marketing campaign.\n",
        "\n",
        "- Dataset issue: Highly imbalanced (5% positive, 95% negative).\n",
        "\n",
        "- Challenge: If we only use accuracy, a model could predict â€œno responseâ€ for everyone and still be 95% accurate â†’ but useless for the business.\n",
        "\n",
        "2.Data Handling\n",
        "\n",
        "- Data Cleaning: Handle missing values, remove duplicates, encode categorical features (One-Hot Encoding/Label Encoding).\n",
        "\n",
        "- Feature Engineering: Create meaningful features (e.g., past purchases, browsing history, campaign history).\n",
        "\n",
        "- Feature Scaling: Apply StandardScaler so features are comparable in magnitude.\n",
        "\n",
        "3.Balancing Classes\n",
        "\n",
        "- Since only 5% respond, we must handle imbalance:\n",
        "\n",
        "- Resampling techniques:\n",
        "\n",
        "- Oversampling the minority class (SMOTE, Random Oversampling).\n",
        "\n",
        "- Undersampling the majority class.\n",
        "\n",
        "- Class Weights in Logistic Regression:\n",
        "\n",
        "- Use class_weight='balanced' in sklearn to give higher weight to minority class.\n",
        "\n",
        "4.Model Building (Logistic Regression)\n",
        "\n",
        "- Use Logistic Regression with regularization (L1/L2).\n",
        "\n",
        "5.Hyperparameter Tuning\n",
        "\n",
        "- Use GridSearchCV to tune:\n",
        "\n",
        "- C (regularization strength).\n",
        "\n",
        "- penalty (L1, L2).\n",
        "\n",
        "- class_weight (balanced vs. none).\n",
        "\n",
        "- Perform k-fold cross-validation to avoid overfitting.\n",
        "\n",
        "6.Evaluation Metrics\n",
        "\n",
        "- Since accuracy is misleading in imbalanced data, use:\n",
        "\n",
        "- Precision: Out of predicted responders, how many actually respond.\n",
        "\n",
        "- Recall (Sensitivity): Out of all actual responders, how many were identified.\n",
        "\n",
        "- F1 Score: Balance between precision and recall.\n",
        "\n",
        "- ROC-AUC: Measures separability of classes.\n",
        "\n",
        "- PR-AUC (Precision-Recall Curve): More informative than ROC when dataset is highly imbalanced.\n",
        "\n",
        "7.Business Perspective\n",
        "\n",
        "- High Recall ensures we donâ€™t miss too many potential responders.\n",
        "\n",
        "- High Precision ensures we donâ€™t waste resources targeting uninterested customers.\n",
        "\n",
        "- Balance depends on business strategy:\n",
        "\n",
        "- If campaign cost is low â†’ prioritize recall.\n",
        "\n",
        "- If campaign cost is high â†’ prioritize precision.\n",
        "\n",
        "8.Approach Summary\n",
        "\n",
        "- Clean & preprocess dataset.\n",
        "\n",
        "- Scale features with StandardScaler.\n",
        "\n",
        "- Handle imbalance (SMOTE / class_weight).\n",
        "\n",
        "- Train Logistic Regression with regularization.\n",
        "\n",
        "- Tune hyperparameters (C, penalty, class_weight).\n",
        "\n",
        "- Evaluate using Precision, Recall, F1, ROC-AUC, PR-AUC.\n",
        "\n",
        "- Select model based on business trade-off (precision vs. recall).\n",
        "\n",
        "What this program does\n",
        "\n",
        "- Creates a synthetic imbalanced dataset (95% vs 5%).\n",
        "\n",
        "- Splits into train/test sets.\n",
        "\n",
        "- Applies feature scaling.\n",
        "\n",
        "- Uses SMOTE to balance classes in training data.\n",
        "\n",
        "- Tunes Logistic Regression with GridSearchCV (C, penalty, class_weight).\n",
        "\n",
        "- Evaluates model with classification report, confusion matrix, ROC-AUC.\n",
        "\n",
        "Python Code: Logistic Regression on Imbalanced Dataset"
      ],
      "metadata": {
        "id": "IpCjET_-v2QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 1. Create an imbalanced dataset (95% class 0, 5% class 1)\n",
        "X, y = make_classification(n_samples=5000, n_features=10, n_informative=6,\n",
        "                           n_redundant=2, n_classes=2, weights=[0.95, 0.05],\n",
        "                           random_state=42)\n",
        "\n",
        "# Convert to DataFrame (optional, for readability)\n",
        "df = pd.DataFrame(X, columns=[f\"Feature_{i}\" for i in range(1, 11)])\n",
        "df[\"Target\"] = y\n",
        "\n",
        "print(\"Original Class Distribution:\\n\", df[\"Target\"].value_counts())\n",
        "\n",
        "# 2. Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# 3. Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Balance classes using SMOTE (oversampling minority class)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Balanced Class Distribution (after SMOTE):\\n\", pd.Series(y_train_balanced).value_counts())\n",
        "\n",
        "# 5. Logistic Regression with GridSearchCV for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear'],  # liblinear supports l1 & l2\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='f1')\n",
        "grid.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "print(\"\\nBest Parameters:\", grid.best_params_)\n",
        "\n",
        "# 6. Evaluate on test set\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:,1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTRBvTYUxcja",
        "outputId": "3f14fa1c-b68b-452c-fe0f-e1e92072032e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Class Distribution:\n",
            " Target\n",
            "0    4724\n",
            "1     276\n",
            "Name: count, dtype: int64\n",
            "Balanced Class Distribution (after SMOTE):\n",
            " 1    3779\n",
            "0    3779\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Best Parameters: {'C': 0.01, 'class_weight': None, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.76      0.85       945\n",
            "           1       0.14      0.67      0.23        55\n",
            "\n",
            "    accuracy                           0.75      1000\n",
            "   macro avg       0.56      0.72      0.54      1000\n",
            "weighted avg       0.93      0.75      0.82      1000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[716 229]\n",
            " [ 18  37]]\n",
            "ROC-AUC Score: 0.7777777777777778\n"
          ]
        }
      ]
    }
  ]
}