{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM & Naive Bayes | Assignment**"
      ],
      "metadata": {
        "id": "TRwKd35ftSPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is a Support Vector Machine (SVM), and how does it work?**\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm primarily used for classification tasks, though it can also be applied to regression (SVR). It works by finding the optimal hyperplane that separates data points of different classes with the maximum margin.\n",
        "\n",
        "How SVM Works:\n",
        "\n",
        "1.Hyperplane Concept:\n",
        "\n",
        "- In a 2D space, the hyperplane is a line that separates two classes. In higher dimensions, it becomes a plane or a hyperplane.\n",
        "\n",
        "- The goal is to position the hyperplane so that the distance between the closest points of each class (called support vectors) and the hyperplane is maximized. This distance is called the margin.\n",
        "\n",
        "2.Support Vectors:\n",
        "\n",
        "- Support vectors are the data points that lie closest to the hyperplane.\n",
        "\n",
        "- These points are crucial because they define the position and orientation of the hyperplane.\n",
        "\n",
        "3.Linear vs Non-Linear SVM:\n",
        "\n",
        "- Linear SVM: Used when data is linearly separable.\n",
        "\n",
        "- Non-linear SVM: Uses kernel functions (like RBF, polynomial, or sigmoid) to map data into a higher-dimensional space where it can be linearly separated.\n",
        "\n",
        "4.Kernel Trick:\n",
        "\n",
        "- SVM uses kernels to handle non-linear data efficiently without explicitly computing the coordinates in higher dimensions.\n",
        "\n",
        "5.Common kernels:\n",
        "\n",
        "- Linear Kernel: Simple linear separation.\n",
        "\n",
        "- Polynomial Kernel: Handles polynomial relationships.\n",
        "\n",
        "- RBF (Radial Basis Function) Kernel: Maps data into infinite-dimensional space to handle complex boundaries.\n",
        "\n",
        "6.Optimization:\n",
        "\n",
        "- SVM solves a convex optimization problem to maximize the margin while minimizing classification errors.\n",
        "\n",
        "- A regularization parameter C controls the trade-off between maximizing the margin and minimizing misclassification.\n",
        "\n",
        "Summary:\n",
        "\n",
        "- SVM finds the best separating boundary (hyperplane) between classes using support vectors, maximizing the margin, and optionally applying kernels for non-linear data. It is powerful for both high-dimensional data and small-to-medium datasets."
      ],
      "metadata": {
        "id": "uJ0VF5sUtjko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Explain the difference between Hard Margin and Soft Margin SVM.**\n",
        "\n",
        "Support Vector Machines (SVM) can be implemented using Hard Margin or Soft Margin, depending on the data and tolerance for misclassification. Here‚Äôs the difference:\n",
        "\n",
        "1.Hard Margin SVM\n",
        "\n",
        "- Definition: Assumes the data is perfectly linearly separable. The algorithm finds a hyperplane that separates the classes without any errors.\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "- No data points are allowed to lie inside the margin or on the wrong side of the hyperplane.\n",
        "\n",
        "- Maximizes the margin strictly between the two classes.\n",
        "\n",
        "- Works well only if the data is clean and noise-free.\n",
        "\n",
        "Limitation:\n",
        "\n",
        "- Sensitive to outliers or noisy data. Even a single misclassified point can make it impossible to find a solution.\n",
        "\n",
        "2.Soft Margin SVM\n",
        "\n",
        "- Definition: Allows some misclassifications or violations of the margin to handle non-linearly separable or noisy data.\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "- Introduces a slack variable (Œæ) to allow some points to lie inside the margin or on the wrong side.\n",
        "\n",
        "- Uses a regularization parameter (C) to balance margin maximization and classification error:\n",
        "\n",
        "- High C ‚Üí less tolerance for misclassification (behaves more like Hard Margin).\n",
        "\n",
        "- Low C ‚Üí more tolerance, wider margin, more misclassifications allowed.\n",
        "\n",
        "Advantage:\n",
        "\n",
        "- More robust to noise and outliers.\n",
        "\n",
        "- Can handle overlapping or non-linearly separable data.\n",
        "\n",
        "Summary Table :\n",
        "\n",
        "| Feature             | Hard Margin SVM    | Soft Margin SVM               |\n",
        "| ------------------- | ------------------ | ----------------------------- |\n",
        "| Data requirement    | Linearly separable | Can be non-linearly separable |\n",
        "| Misclassification   | Not allowed        | Allowed (controlled by C)     |\n",
        "| Margin flexibility  | Fixed              | Flexible                      |\n",
        "| Robustness to noise | Low                | High                          |\n"
      ],
      "metadata": {
        "id": "32fttxZbuc0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use cases.**\n",
        "\n",
        "Kernel Trick in SVM:\n",
        "\n",
        "- Support Vector Machines (SVM) work by finding a hyperplane that separates data points of different classes. However, not all datasets are linearly separable in their original feature space. The kernel trick is a technique that allows SVM to operate in a higher-dimensional space without explicitly computing the coordinates of the data in that space. Instead, it uses a kernel function to compute the inner product of transformed features directly. This makes it computationally efficient to handle non-linear decision boundaries.\n",
        "\n",
        "- Mathematically, if\n",
        "$œï(x)$ is a mapping to a higher-dimensional space, the kernel trick uses:\n",
        "\n",
        "- $K(xi‚Äã,xj‚Äã)=‚ü®œï(xi‚Äã),œï(xj‚Äã)‚ü©$\n",
        "\n",
        "- where\n",
        "K is the kernel function.\n",
        "\n",
        "- Example of a Kernel: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "- The RBF kernel is defined as:\n",
        "\n",
        "- $$\n",
        "K(x_i, x_j) = \\exp\\left(-\\gamma \\|x_i - x_j\\|^2\\right)$$\n",
        "\n",
        "- Use Case: The RBF kernel is useful when the relationship between class labels and features is non-linear. It maps the data into an infinite-dimensional space where a linear separation becomes possible.\n",
        "\n",
        "- Example Scenario: Classifying points in a 2D XOR problem, where classes are diagonally opposite, cannot be separated by a straight line in the original 2D space. Using the RBF kernel, SVM can find a non-linear boundary to separate the classes effectively.\n",
        "\n",
        "Example Use Case: RBF Kernel\n",
        "\n",
        "- Imagine a 2D dataset with points forming concentric circles:\n",
        "\n",
        "- Inner circle = Class A\n",
        "\n",
        "- Outer circle = Class B\n",
        "\n",
        "- A linear SVM cannot separate these classes with a straight line.\n",
        "Using the RBF kernel, the data is mapped into a higher-dimensional space where it becomes linearly separable, and SVM can find a hyperplane to distinguish the classes.\n",
        "\n",
        "Intuitive Analogy\n",
        "\n",
        "- Think of the kernel trick as lifting a tangled string off a table:\n",
        "\n",
        "- On the table (2D space), the string loops around itself, impossible to cut with a straight line.\n",
        "\n",
        "- Lift it into 3D space (kernel mapping), and suddenly the string becomes untangled.\n",
        "\n",
        "- Now you can ‚Äúcut‚Äù it with a flat plane (hyperplane) ‚Äî which corresponds to a non-linear separation in 2D.\n"
      ],
      "metadata": {
        "id": "X18jq8j7vUIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?**\n",
        "\n",
        "Na√Øve Bayes Classifier\n",
        "\n",
        "- A Na√Øve Bayes (NB) classifier is a probabilistic machine learning algorithm used for classification tasks. It is based on Bayes‚Äô Theorem, which calculates the probability of a class given the observed features.\n",
        "\n",
        "- Bayes‚Äô Theorem formula:\n",
        "\n",
        "- $ P(C \\mid X) = \\frac{P(X \\mid C) \\cdot P(C)}{P(X)}$\n",
        "\n",
        "Where:\n",
        "\n",
        "- ùëÉ(ùê∂‚à£ùëã)= Posterior probability of class ùê∂ given feature vector ùëã\n",
        "\n",
        "- ùëÉ(ùëã‚à£ùê∂)= Likelihood of feature vector ùëã given class ùê∂\n",
        "\n",
        "- ùëÉ(ùê∂) = Prior probability of class\n",
        "\n",
        "- ùëÉ(ùëã) = Probability of feature vector ùëã (can be ignored for classification since it‚Äôs constant across classes)\n",
        "\n",
        "- The Na√Øve Bayes classifier predicts the class with the highest posterior probability.\n",
        "\n",
        "Why is it called ‚ÄúNa√Øve‚Äù?\n",
        "\n",
        "- It is called na√Øve because it assumes that all features are independent of each other given the class.\n",
        "\n",
        "- In real-world data, features are often correlated, but NB ignores these correlations.\n",
        "\n",
        "- This ‚Äúna√Øve‚Äù assumption simplifies computations and allows the classifier to work efficiently on large datasets.\n",
        "\n",
        "Types of Na√Øve Bayes Classifiers\n",
        "\n",
        "- Gaussian Na√Øve Bayes: Assumes features follow a Gaussian (normal) distribution.\n",
        "\n",
        "- Multinomial Na√Øve Bayes: Used for discrete data, e.g., word counts in text classification.\n",
        "\n",
        "- Bernoulli Na√Øve Bayes: Used for binary features (0 or 1), e.g., presence/absence of a word.\n",
        "\n",
        "Example Use Case\n",
        "\n",
        "Spam Email Detection:\n",
        "\n",
        "- Features: Words in the email (e.g., ‚Äúfree‚Äù, ‚Äúoffer‚Äù, ‚Äúwin‚Äù)\n",
        "\n",
        "- Classes: Spam or Not Spam\n",
        "\n",
        "- Na√Øve Bayes calculates the probability of an email being spam based on the presence of words.\n",
        "\n",
        "Even though the words may not be fully independent, NB often performs very well in text classification due to its simplicity and efficiency."
      ],
      "metadata": {
        "id": "ebSfm31KxwfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants. When would you use each one?**\n",
        "\n",
        "1.Gaussian Na√Øve Bayes (GNB)\n",
        "\n",
        "Description:\n",
        "\n",
        "- Assumes that the continuous features of the data follow a Gaussian (normal) distribution.\n",
        "\n",
        "- The likelihood $P(xi‚à£C)$ for a feature ùë•ùëñ is calculated using the Gaussian probability density function:\n",
        "\n",
        "- $P(x_i \\mid C) = \\frac{1}{\\sqrt{2\\pi \\sigma_C^2}} \\exp\\left( -\\frac{(x_i - \\mu_C)^2}{2\\sigma_C^2} \\right)$\n",
        "\n",
        "where\n",
        "\n",
        "- ùúáùê∂ and ùúé2C are the mean and variance of feature ùë•ùëñ\n",
        "\n",
        "- for class ùê∂.\n",
        "\n",
        "Use Case:\n",
        "\n",
        "- Used when features are continuous numerical values.\n",
        "\n",
        "- Example: Predicting whether a patient has a disease based on continuous features like blood pressure, cholesterol level, or age.\n",
        "\n",
        "2.Multinomial Na√Øve Bayes (MNB)\n",
        "\n",
        "Description:\n",
        "\n",
        "- Designed for discrete count data, especially feature vectors representing frequencies.\n",
        "\n",
        "- Computes the probability of features (e.g., word counts) given a class using a multinomial distribution.\n",
        "\n",
        "- $P(x_i \\mid C) = \\frac{\\text{count of feature } i \\text{ in class } C + \\alpha}{\\text{total count of all features in class } C + \\alpha \\cdot n}$\n",
        "\n",
        "where\n",
        "\n",
        "- ùõº is the Laplace smoothing parameter and ùëõ is the number of features.\n",
        "\n",
        "Use Case:\n",
        "\n",
        "- Commonly used in text classification tasks where features represent word counts.\n",
        "\n",
        "- Example: Spam detection in emails, sentiment analysis, or document categorization.\n",
        "\n",
        "3.Bernoulli Na√Øve Bayes (BNB)\n",
        "\n",
        "Description:\n",
        "\n",
        "- Designed for binary/boolean features (0 or 1).\n",
        "\n",
        "- Focuses on presence or absence of a feature rather than frequency.\n",
        "\n",
        "- $P(x_i \\mid C) = P(f_i = 1 \\mid C)^{x_i} \\cdot (1 - P(f_i = 1 \\mid C))^{1 - x_i}$\n",
        "\n",
        "Use Case:\n",
        "\n",
        "- Used when data features are binary indicators.\n",
        "\n",
        "- Example: Text classification based on whether a word appears in a document or not, click prediction (clicked/not clicked), or presence/absence of certain symptoms.\n",
        "\n",
        "Summary Table:\n",
        "\n",
        "| Variant        | Feature Type    | Use Case Example                     |\n",
        "| -------------- | --------------- | ------------------------------------ |\n",
        "| Gaussian NB    | Continuous      | Predicting disease from medical data |\n",
        "| Multinomial NB | Discrete counts | Spam detection, sentiment analysis   |\n",
        "| Bernoulli NB   | Binary features | Text classification (word presence)  |\n"
      ],
      "metadata": {
        "id": "7OQqhAkozlK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Write a Python program to:**\n",
        "\n",
        "**‚óè Load the Iris dataset**\n",
        "\n",
        "**‚óè Train an SVM Classifier with a linear kernel**\n",
        "\n",
        "**‚óè Print the model's accuracy and support vectors.**\n",
        "\n",
        "Explanation:\n",
        "\n",
        "1. Load Dataset: load_iris() provides 150 samples with 4 features each (sepal/petal length and width).\n",
        "\n",
        "2. Train/Test Split: We split data into 80% training and 20% testing.\n",
        "\n",
        "3. SVM Training: We use a linear kernel because Iris data is mostly linearly separable.\n",
        "\n",
        "4. Accuracy: Evaluates how well the model predicts unseen test data.\n",
        "\n",
        "5. Support Vectors: SVM identifies the data points that lie closest to the decision boundary ‚Äî these are critical for defining the hyperplane."
      ],
      "metadata": {
        "id": "vsrOGLF-2VxC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpXQKidtrDGn",
        "outputId": "c679088f-3953-4ffc-8624-d855f5073404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data       # Features\n",
        "y = iris.target     # Labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train an SVM classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Step 5: Print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Print the support vectors\n",
        "print(\"Support Vectors:\")\n",
        "print(svm_model.support_vectors_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Write a Python program to:**\n",
        "\n",
        "**‚óè Load the Breast Cancer dataset**\n",
        "\n",
        "**‚óè Train a Gaussian Na√Øve Bayes model**\n",
        "\n",
        "**‚óè Print its classification report including precision, recall, and F1-score.**\n",
        "\n",
        "Explanation:\n",
        "\n",
        "1. Load Dataset: load_breast_cancer() provides 569 samples with 30 numerical features describing tumor properties.\n",
        "\n",
        "2. Train/Test Split: 80% for training and 20% for testing.\n",
        "\n",
        "3. Gaussian Na√Øve Bayes: Assumes continuous features follow a Gaussian distribution.\n",
        "\n",
        "4. Classification Report: Includes precision, recall, F1-score, and support for each class.\n",
        "\n",
        "5. Precision: How many predicted positives are correct.\n",
        "\n",
        "6. Recall: How many actual positives are correctly predicted.\n",
        "\n",
        "7. F1-score: Harmonic mean of precision and recall."
      ],
      "metadata": {
        "id": "wWgM1y8R3L0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data      # Features\n",
        "y = data.target    # Labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train a Gaussian Na√Øve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Step 5: Print the classification report\n",
        "report = classification_report(y_test, y_pred, target_names=data.target_names)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k6rVmjl3koD",
        "outputId": "dfd0e4e4-8b60-4773-f7c5-532392573713"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Write a Python program to:**\n",
        "\n",
        "**‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.**\n",
        "\n",
        "**‚óè Print the best hyperparameters and accuracy.**\n",
        "\n",
        "Explanation:\n",
        "\n",
        "1. Wine Dataset: Contains 178 samples with 13 chemical features of wines categorized into 3 classes.\n",
        "\n",
        "2. SVM with RBF Kernel: gamma is relevant for RBF kernel as it defines the influence of a single training point.\n",
        "\n",
        "3. GridSearchCV:\n",
        "\n",
        "- Exhaustively searches over the grid of hyperparameters C (regularization) and gamma (kernel coefficient).\n",
        "\n",
        "- cv=5 means 5-fold cross-validation.\n",
        "\n",
        "4. Best Hyperparameters: grid_search.best_params_ gives the combination of C and gamma that achieves the highest accuracy on validation folds.\n",
        "\n",
        "5. Test Accuracy: Evaluates performance of the tuned model on unseen test data."
      ],
      "metadata": {
        "id": "0pmt5jdl3naT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define the SVM classifier\n",
        "svm = SVC(kernel='rbf')  # Using RBF kernel as gamma is relevant\n",
        "\n",
        "# Step 4: Set up the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "# Step 5: Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Step 7: Evaluate the model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFn3b2aa4Q-i",
        "outputId": "88a54d45-d784-426d-bc91-a766db46f8e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001}\n",
            "Test Set Accuracy: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Write a Python program to:**\n",
        "\n",
        "**‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).**\n",
        "\n",
        "**‚óè Print the model's ROC-AUC score for its predictions.**\n",
        "\n",
        "Explanation:\n",
        "\n",
        "1. Dataset:\n",
        "\n",
        "- fetch_20newsgroups provides a large collection of news articles.\n",
        "\n",
        "- For simplicity, we selected 3 categories.\n",
        "\n",
        "2. Text Vectorization:\n",
        "\n",
        "- TfidfVectorizer converts raw text into numerical features based on word frequency and importance.\n",
        "\n",
        "3. Na√Øve Bayes Model:\n",
        "\n",
        "- MultinomialNB is suitable for discrete count data like TF-IDF features.\n",
        "\n",
        "4. ROC-AUC Score:\n",
        "\n",
        "- For multi-class problems, we binarize labels and use One-vs-Rest (OvR) strategy.\n",
        "\n",
        "- average='macro' computes the average ROC-AUC across classes."
      ],
      "metadata": {
        "id": "O66O1woj4Whd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load a subset of the 20 Newsgroups dataset\n",
        "categories = ['alt.atheism', 'comp.graphics', 'sci.space']  # Selecting 3 categories for simplicity\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Step 2: Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Step 3: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train a Multinomial Na√Øve Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make probability predictions for ROC-AUC calculation\n",
        "y_prob = nb_model.predict_proba(X_test)\n",
        "\n",
        "# Step 6: Binarize labels for multi-class ROC-AUC\n",
        "y_test_bin = label_binarize(y_test, classes=np.arange(len(categories)))\n",
        "\n",
        "# Step 7: Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test_bin, y_prob, average='macro', multi_class='ovr')\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX8BTXdu4VN8",
        "outputId": "a7b0dcc9-713b-42f8-ba57-b261bc145c98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Imagine you‚Äôre working as a data scientist for a company that handles email communications. Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:**\n",
        "\n",
        "**‚óè Text with diverse vocabulary**\n",
        "\n",
        "**‚óè Potential class imbalance (far more legitimate emails than spam)**\n",
        "\n",
        "**‚óè Some incomplete or missing data\n",
        "Explain the approach you would take to:**\n",
        "\n",
        "**‚óè Preprocess the data (e.g. text vectorization, handling missing data)**\n",
        "\n",
        "**‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes)**\n",
        "\n",
        "**‚óè Address class imbalance**\n",
        "\n",
        "**‚óè Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.**\n",
        "\n",
        "1.Preprocessing the Data\n",
        "\n",
        "a) Handling Text Data\n",
        "\n",
        "- Emails are unstructured text, so we need to convert them into numerical features:\n",
        "\n",
        "1. Text Cleaning:\n",
        "\n",
        "- Remove punctuation, numbers, and special characters.\n",
        "\n",
        "- Convert all text to lowercase.\n",
        "\n",
        "- Remove stop words (common words like ‚Äúthe‚Äù, ‚Äúis‚Äù that carry little meaning).\n",
        "\n",
        "2. Tokenization & Vectorization:\n",
        "\n",
        "- Use TF-IDF Vectorization (TfidfVectorizer) or CountVectorizer to convert text into numerical features.\n",
        "\n",
        "- TF-IDF is preferred because it gives higher weight to words that are informative and lower weight to common words.\n",
        "\n",
        "b) Handling Missing Data\n",
        "\n",
        "- Check for missing email text or labels.\n",
        "\n",
        "- Replace missing text with an empty string or a placeholder.\n",
        "\n",
        "- Drop rows with missing labels because they cannot be used for supervised learning.\n",
        "\n",
        "2.Choosing an Appropriate Model\n",
        "\n",
        "- Options: SVM vs. Na√Øve Bayes\n",
        "\n",
        "| Model           | Pros                                                                            | Cons                                                            |\n",
        "| --------------- | ------------------------------------------------------------------------------- | --------------------------------------------------------------- |\n",
        "| **Na√Øve Bayes** | Fast, handles high-dimensional text data well, robust to irrelevant features    | Assumes independence of words, may not capture complex patterns |\n",
        "| **SVM**         | Can handle non-linear boundaries (with kernels), good for high-dimensional data | Slower to train on very large datasets, requires careful tuning |\n",
        "\n",
        "Recommendation:\n",
        "\n",
        "- For text classification with high-dimensional sparse features (TF-IDF vectors), Multinomial Na√Øve Bayes is often preferred.\n",
        "\n",
        "- It works well even when the independence assumption is not strictly true, and it is very fast for large datasets.\n",
        "\n",
        "- SVM can also be used if very high accuracy is required and computational resources allow.\n",
        "\n",
        "3.Addressing Class Imbalance\n",
        "\n",
        "- Spam detection datasets often have many more legitimate emails than spam emails, which can bias the model. Approaches to address this:\n",
        "\n",
        "1. Resampling:\n",
        "\n",
        "- Oversample minority class (spam) using techniques like SMOTE.\n",
        "\n",
        "- Undersample majority class (ham) to balance the dataset.\n",
        "\n",
        "2. Class Weights:\n",
        "\n",
        "- In SVM or other classifiers, set class_weight='balanced' to penalize misclassifying the minority class more.\n",
        "\n",
        "3. Threshold Tuning:\n",
        "\n",
        "- Adjust decision thresholds to improve detection of spam emails, even if it slightly increases false positives.\n",
        "\n",
        "4.Evaluating Performance\n",
        "\n",
        "Metrics:\n",
        "\n",
        "- Accuracy: Not sufficient alone because of imbalance.\n",
        "\n",
        "- Precision (Spam Prediction Accuracy): High precision ensures legitimate emails are rarely flagged as spam.\n",
        "\n",
        "- Recall (Spam Detection Rate): High recall ensures most spam emails are caught.\n",
        "\n",
        "- F1-Score: Harmonic mean of precision and recall, balances the trade-off.\n",
        "\n",
        "- ROC-AUC Score: Measures overall separability of spam vs. non-spam.\n",
        "\n",
        "Example:\n",
        "\n",
        "- A spam classifier with high recall but moderate precision may flag some legitimate emails, but almost no spam is missed.\n",
        "\n",
        "5.Business Impact\n",
        "\n",
        "1. Implementing an effective spam classifier brings significant value:\n",
        "\n",
        "2. Improved Productivity: Employees spend less time sorting spam emails.\n",
        "\n",
        "3. Reduced Security Risk: Spam emails often carry phishing attempts or malware. Detecting them reduces the risk of data breaches.\n",
        "\n",
        "4. Customer Trust: Ensures critical emails are delivered correctly while keeping spam out of inboxes.\n",
        "\n",
        "5. Operational Efficiency: Automates email filtering, reducing the need for manual review.\n",
        "\n",
        "Key Takeaway: A well-designed spam detection system balances accuracy, speed, and reliability, enhancing security and efficiency while maintaining user satisfaction.\n",
        "\n",
        "Workflow Explanation:\n",
        "\n",
        "1. Dataset:\n",
        "\n",
        "- Used fetch_20newsgroups as a sample dataset.\n",
        "\n",
        "- Simulated missing emails for preprocessing demonstration.\n",
        "\n",
        "2. Preprocessing:\n",
        "\n",
        "- Filled missing email text with empty strings.\n",
        "\n",
        "- Converted raw text to TF-IDF features for numeric representation.\n",
        "\n",
        "3. Class Imbalance:\n",
        "\n",
        "- Calculated class weights, though MultinomialNB doesn‚Äôt directly use them, it‚Äôs useful if switching to other classifiers like SVM.\n",
        "\n",
        "4. Model:\n",
        "\n",
        "- Multinomial Na√Øve Bayes is ideal for text classification with word count/TF-IDF features.\n",
        "\n",
        "5. Evaluation:\n",
        "\n",
        "- Classification report gives precision, recall, F1-score, and support.\n",
        "\n",
        "- ROC-AUC measures model's ability to separate classes.\n",
        "\n",
        "Python Implementation :"
      ],
      "metadata": {
        "id": "JFHrwzDd5F52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Load or simulate the dataset\n",
        "# -----------------------------\n",
        "# For demonstration, we use a subset of 20 Newsgroups to simulate spam/ham\n",
        "categories = ['rec.autos', 'sci.space']  # 'rec.autos' = Not Spam, 'sci.space' = Spam\n",
        "emails = fetch_20newsgroups(subset='all', categories=categories)\n",
        "X = emails.data\n",
        "y = emails.target\n",
        "\n",
        "# Convert to DataFrame to simulate missing data\n",
        "df = pd.DataFrame({'text': X, 'label': y})\n",
        "df.loc[5, 'text'] = None  # Simulate missing email\n",
        "df.loc[10, 'text'] = None\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Preprocess the text\n",
        "# -----------------------------\n",
        "# Fill missing email text with empty string\n",
        "df['text'].fillna('', inplace=True)\n",
        "X = df['text'].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Vectorize text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Train/Test split\n",
        "# -----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Handle class imbalance (optional)\n",
        "# -----------------------------\n",
        "# Compute class weights\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Train a Multinomial Na√Øve Bayes model\n",
        "# -----------------------------\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Make predictions\n",
        "# -----------------------------\n",
        "y_pred = nb_model.predict(X_test)\n",
        "y_prob = nb_model.predict_proba(X_test)\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Evaluate the model\n",
        "# -----------------------------\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=categories))\n",
        "\n",
        "# For ROC-AUC (multi-class / binary)\n",
        "y_test_bin = label_binarize(y_test, classes=classes)\n",
        "roc_auc = roc_auc_score(y_test_bin, y_prob[:,1], average='macro')\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpEGDJps67wN",
        "outputId": "9c557faf-4b4c-4b8a-e3e5-47d3aed548ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3621335111.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['text'].fillna('', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   rec.autos       0.98      0.99      0.99       198\n",
            "   sci.space       0.99      0.98      0.99       198\n",
            "\n",
            "    accuracy                           0.99       396\n",
            "   macro avg       0.99      0.99      0.99       396\n",
            "weighted avg       0.99      0.99      0.99       396\n",
            "\n",
            "ROC-AUC Score: 0.9998\n"
          ]
        }
      ]
    }
  ]
}